## 1. 基于 LlamaIndex 构建 RAG 应用
### 1.1 大模型的通用问题
- 问题：
 - 领域知识缺乏：训练模型时无法囊括专业领域知识
 - 信息过时：预训练模型，训练成本高，无法短时间再次训练
- 影响：
 - 幻觉问题：看似合理的错误答案，数学方面体现的最明显
### 1.2 RAG 的产生背景
- 为了解决生成式AI应用中模型面临知识更新滞后、幻觉等问题，`Facebook AI`在`2020`提出了`RAG (Retrieval-Augmented-Generation)`技术，提升生成内容的准确性和相关性
### 1.3 RAG 的一般工作流程
- 问题 -> 检索嵌入文本 -> Prompt组合 -> 生成回答 
- 朴素`RAG`的一般流程包括以下三个核心步骤：
 - 索引：
  - 处理文档，提取文本并切分为文本块
  - 将文本块进行嵌入向量化，并将向量存储在向量数据库中
 - 检索：
  - 将用户查询转化为向量表示，将查询映射到与知识库内容相同的向量空间中
  - 通过相似度匹配从向量数据库中检索出最相关的文本块，查询到最相关的前`K`个文本块
 - 生成：
  - 将检索到的相关文本块与原始查询结合，构成提示词
  - 将提示词输入大语言模型，生成具备领域知识和私有信息的精确内容
- `RAG`与微调的区别
 - `RAG`
  - 外部知识整合：擅长高效整合外部知识，尤其适用于需要频繁处理实时信息或依赖外部知识的场景。
  - 实时性：能够实时利用最新信息，生成上下文相关且准确的答案。
  - 可解释性：生成的答案通常具备可解释性，因为答案基于检索到的相关文本块。
  - 适用场景：适用于需要动态响应、频繁更新外部知识的场景，如问答系统、实时信息处理等。
 - 微调
  - 领域能力优化：通过使用特定领域的数据对模型进行深度优化，提升模型在特定任务或领域中的推理能力。
  - 稳定性：适用于需求稳定、领域知识固定且不需要频繁更新知识库的场景。
  - 专业性和一致性：确保输出内容的专业性和一致性，适合固定领域内的深度优化与推理。
  - 适用场景：适用于特定领域内的任务，如法律、医疗、金融等专业领域。
### 1.4 基于 LlamaIndex 实现 RAG
- 如何看待`LlamaIndex`？
 - `LLMs` -> 桥梁 -> 数据源
 - 设计哲学：逐步披露复杂性
 - 结构清晰
- 学 Python 最重要的两个点
 - 如何用好一个框架
 - 如何编写这样一个框架
- 多去敲代码，多去实践，多去看代码